# -*- coding: utf-8 -*-
"""Eventos_CLTV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NHHv2HkiPYP53GN2_ow_G60PMeVeElzA
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
# %matplotlib inline
from scipy import stats
import pandas as pd 
import multiprocessing
import random

from matplotlib import rc
from sklearn.preprocessing import scale
from sklearn.preprocessing import OneHotEncoder
import timeit
import itertools
import seaborn as sns
from collections import Counter
import operator

from google.colab import drive
drive.mount('/content/drive')

"""# LOAD EVENTOS"""

txt_event = '/content/drive/My Drive/TFM/03_DATASETS/eventos_2.rpt'
widths=[10,39,12,16,12,39,20,10,41,16,50,39,17,18,39,41,41,41,41,41,30,39] 

dfevent = pd.read_fwf(txt_event, widths=widths, header=1, index_col=None, index=True)
rowcl,colcl = dfevent.shape
dfevent=dfevent[0:(rowcl-3)]

new_header = ['TipoEvento','CodigoEvento','FechaEvento','UsuarioEvento','HoraEvento','ClienteEvento','CodigoPostalEvento','PaísEvento','RepresentanteEvento','TipoPortesEvento','FormaPagoEvento','PlazoPagoEvento','SkuArticuloEvento','TipoArticuloEvento','FamiliaArticuloEvento','SubfamiliaArticuloEvento','CantidadArticuloEvento','AlmacenArticuloEvento','TarifaArticuloEvento','DescuentoArticuloEvento','MotivoEvento','CosteEvento']
dfevent.columns = new_header
dfevent

#vamos a utilizar solo los últimos 5 años
dfevent = dfevent[(dfevent['FechaEvento'] >= '2016-01-01') &
          (dfevent['FechaEvento'] <= '2020-12-31')]

#eliminamos los eventos que sean una oferta
indexNames = dfevent[ dfevent['TipoEvento'] == 'OFERTA' ].index
# Delete these row indexes from dataFrame
dfevent.drop(indexNames , inplace=True)

events=dfevent.sort_values(by=['FechaEvento'])

events['FechaEvento']

events.info()

"""# CHANGE CLIENT CODE BY CLIENT NAME"""

# Load the dataset CLIENTES
txt_client = '/content/drive/My Drive/TFM/03_DATASETS/clientes.rpt'

widths = [40, 31, 16, 21, 12, 51, 17, 40, 40, 15, 50]

dfclient = pd.read_fwf(txt_client, widths=widths, header=1, index_col=None, index=True)
rowcl,colcl = dfclient.shape
dfclient=dfclient[0:(rowcl-3)]

new_header = ['CodigoCliente','NombreCliente','NIFCliente','CodigoPostalCliente','PaisCliente','SegmentoCliente','FechaAltaCliente','RepresentanteCliente','AreaCliente','MercadoCliente','GrupoDescuentoCliente']

dfclient.columns = new_header
clientes = dfclient

clientes.head()

id_nombre = clientes[['CodigoCliente','NombreCliente','SegmentoCliente']]

#remove the last nmber and the point of the string index
events['ClienteEvento'] = events['ClienteEvento'].map(lambda x: str(x)[:-2])

events = events.merge(id_nombre,left_on='ClienteEvento', right_on='CodigoCliente')

events

"""# CHECK POINT"""

#!pip install pandas_profiling==2.6.0

#import pandas as pd
#from pandas_profiling import ProfileReport

#report = ProfileReport(events, minimal=True, title='Pandas Profiling Report', html={'style':{'full_width':True}})
#report

events['CantidadArticuloEvento'] = events['CantidadArticuloEvento'].astype(float)
events['TarifaArticuloEvento'] = events['TarifaArticuloEvento'].astype(float)
events['FechaEvento'] =  pd.to_datetime(events['FechaEvento'], format='%Y-%m-%d')
events['ClienteEvento'] = events['ClienteEvento'].astype(str)

eventos=events[['ClienteEvento','NombreCliente','SegmentoCliente','FechaEvento','CodigoEvento','CantidadArticuloEvento','TarifaArticuloEvento','DescuentoArticuloEvento']]#Calulate total purchase

"""# CLTV GENÉRICO"""

#!!!!!! AQUÍ ES DONDE TENDRÉ QUE FILTRAR PARA QUE ME COJA EL SEGMENTO QUE QUIERO ¡¡¡¡¡¡¡
#segmento = eventos.loc[eventos['SegmentoCliente'] == 'SEGMENTO']
#segmento

"""## Start preparing the dataset to compute the CLTV"""

#calculate the total import
eventos['TotalImporte'] = eventos["CantidadArticuloEvento"] * eventos["TarifaArticuloEvento"] * (100 - eventos["DescuentoArticuloEvento"])/100

eventos.info()

eventos.describe()

#no podemos eliminar los eventos con cantidades negativas porque son devoluciones

#Calculate the number of days between the present date and the date of last purchase from each customer.
#Calculate the number of orders for each customer.
#Calculate the sum of the purchase price for each customer.
eventos_data_group=eventos.groupby('ClienteEvento').agg({'FechaEvento': lambda date: (date.max() - date.min()).days, 'CodigoEvento': lambda num: len(num),'CantidadArticuloEvento': lambda quant: quant.sum(),'TotalImporte': lambda price: price.sum()})
eventos_data_group.head()

# Change the name of columns
eventos_data_group.columns=['num_days','num_transactions','num_units','spent_money']
eventos_data_group.head()

#1-> CACLULATE THE Average Order Value
eventos_data_group['avg_order_value']=eventos_data_group['spent_money']/eventos_data_group['num_transactions']
eventos_data_group.head()

#2-> calculate purchase frequency
purchase_frequency=sum(eventos_data_group['num_transactions'])/eventos_data_group.shape[0]

#3->  Repeat Rate
repeat_rate=eventos_data_group[eventos_data_group.num_transactions > 10].shape[0]/eventos_data_group.shape[0]

#4-> Churn Rate
churn_rate=1-repeat_rate
purchase_frequency,repeat_rate,churn_rate

#5-> Profit Margin. Aquí he estimado que el Margen Bruto de Emuca ronda el 35%. Hay que hablarlo con Enrique.
eventos_data_group['profit_margin']=eventos_data_group['spent_money']*0.35
eventos_data_group.head()

# Calculate the Customer Value
eventos_data_group['CLV']=(eventos_data_group['avg_order_value']* purchase_frequency) / churn_rate

#Customer Lifetime Value
eventos_data_group['cust_lifetime_value']=eventos_data_group['CLV']*eventos_data_group['profit_margin']
eventos_data_group.head()

eventos_data_group.describe().astype(int)

"""## Degment the CLTV in 4 groups

Over the total sum of CLTV result:
- 0 - 25% -> DIAMOND (the best)
- 26 - 50% -> EXCLUSIVE
- 51 - 75% -> PREMIUM
- 76 - 100% -> BUSINESS
"""

eventos_data_group=eventos_data_group.sort_values(by=['cust_lifetime_value'], ascending=False)
eventos_data_group

eventos_data_group['percentage'] = eventos_data_group['cust_lifetime_value']/sum(eventos_data_group['cust_lifetime_value'])

eventos_data_group['percentage1'] = (eventos_data_group.cust_lifetime_value.cumsum() / eventos_data_group.cust_lifetime_value.sum()) * 100

eventos_data_group.head(15)

conds = [eventos_data_group.percentage1.between(0,25), eventos_data_group.percentage1.between(26,50),
         eventos_data_group.percentage1.between(51,75), eventos_data_group.percentage1.between(76,100)]


choices = ['DIAMOND','EXCLUSIVE','PREMIUM','BUSINESS']


eventos_data_group['cltv_group'] = np.select(conds,choices, 'BUSINESS')

eventos_data_group

print(eventos_data_group.groupby("cltv_group").size())

eventos_data_group = eventos_data_group.merge(id_nombre,left_on='ClienteEvento', right_on='CodigoCliente')

eventos_data_group = eventos_data_group[['CodigoCliente', 'NombreCliente', 'SegmentoCliente', 'cust_lifetime_value','percentage1', 'cltv_group']]
eventos_data_group.head(20)

eventos_data_group.to_csv('/content/drive/My Drive/TFM/03_DATASETS/CLTV_general.csv',sep=';',decimal=',')